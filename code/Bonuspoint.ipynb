{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('imdb_master.csv',encoding='latin-1')\n",
    "sentences = df['review'].values\n",
    "y = df['label'].values\n",
    "max_review_len= max([len(s.split()) for s in sentences])\n",
    "\n",
    "#tokenizing data\n",
    "tokenizer = Tokenizer(num_words=max_review_len)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "vocab_size= len(tokenizer.word_index)+1\n",
    "sentences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_docs= pad_sequences(sentences,maxlen=max_review_len)\n",
    "\n",
    "\n",
    "#getting the vocabulary of data\n",
    "#sentences = tokenizer.texts_to_matrix(sentences)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "# Number of features\n",
    "# print(input_dim)\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=max_review_len))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(300,activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history=model.fit(X_train,y_train, epochs=5, verbose=True, validation_data=(X_test,y_test), batch_size=256)\n",
    "#evaluating model on training data and calculating loss and accuracy of training data\n",
    "[train_loss, train_acc] = model.evaluate(X_test,y_test)\n",
    "print(\"Evaluation result on Train Data : Loss = {}, accuracy = {}\".format(train_loss, train_acc))\n",
    "# summarize history for accuracy&loss\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuray', 'val_accuracy','loss','val_loss'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Predicting the Value for test sample\n",
    "pred = model.predict_classes(X_test[[2],:])\n",
    "print(\"Actual Prediction\",y_test[1],\"Predicted Prediction\", pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
